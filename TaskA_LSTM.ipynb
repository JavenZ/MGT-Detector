{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "52ed01ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch\n"
     ]
    }
   ],
   "source": [
    "import keras.backend as K\n",
    "\n",
    "print(K.backend())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4a5ad83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import keras\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import TextVectorization\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "640fc742",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Download dataset SubtaskA.jsonl from \n",
    "https://github.com/mbzuai-nlp/M4GT-Bench.\n",
    "\n",
    "Google drive from repo: https://drive.google.com/drive/folders/1hBgW6sgZfz1BK0lVdUu0bZ4HPKSpOMSY\n",
    "Direct link to SubtaskA.jsonl: https://drive.google.com/file/d/1zwSfSKe4-0m2td_cP0Sl2LhKtksvtHlf/view\n",
    "\"\"\"\n",
    "import gdown, os\n",
    "# DATA_PATH = \"C:/Users/Admin/Downloads/SubtaskA.jsonl\"\n",
    "DATA_PATH = \"./datasets/SubtaskA.jsonl\"\n",
    "if not os.path.exists(DATA_PATH):\n",
    "    if not os.path.exists(os.path.dirname(DATA_PATH)):\n",
    "        os.makedirs(os.path.dirname(DATA_PATH))\n",
    "    gdown.download(\"https://drive.google.com/uc?id=1zwSfSKe4-0m2td_cP0Sl2LhKtksvtHlf\", DATA_PATH, quiet=False)\n",
    "\n",
    "# initialize dataframe\n",
    "df = pd.read_json(DATA_PATH, lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cbb3381e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wikihow      36556\n",
      "reddit       33999\n",
      "arxiv        33998\n",
      "wikipedia    31365\n",
      "peerread     16891\n",
      "Name: source, dtype: int64\n",
      "\n",
      "human      65177\n",
      "chatGPT    16892\n",
      "gpt4       14344\n",
      "davinci    14340\n",
      "bloomz     14332\n",
      "dolly      14046\n",
      "cohere     13678\n",
      "Name: model, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df.source.value_counts())\n",
    "print()\n",
    "print(df.model.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d028b763",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "human    65177\n",
      "Name: model, dtype: int64\n",
      "\n",
      "chatGPT    16892\n",
      "gpt4       14344\n",
      "davinci    14340\n",
      "bloomz     14332\n",
      "dolly      14046\n",
      "cohere     13678\n",
      "Name: model, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(df[df.label == 0].model.value_counts())\n",
    "print()\n",
    "print(df[df.label == 1].model.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cbf69435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>We consider a system of many polymers in solut...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>We present a catalog of 66 YSOs in the Serpens...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Spectroscopic Observations of the Intermediate...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>We present a new class of stochastic Lie group...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ALMA as the ideal probe of the solar chromosph...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152804</th>\n",
       "      <td>The main results presented in this dissertati...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152805</th>\n",
       "      <td>Fine-grained sketch-based image retrieval (FG...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152806</th>\n",
       "      <td>We present the derivation of the NNLO two-par...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152807</th>\n",
       "      <td>The principle of optimism in the face of unce...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152808</th>\n",
       "      <td>We consider the setting of prediction with ex...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>152809 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  label\n",
       "0       We consider a system of many polymers in solut...      1\n",
       "1       We present a catalog of 66 YSOs in the Serpens...      1\n",
       "2       Spectroscopic Observations of the Intermediate...      1\n",
       "3       We present a new class of stochastic Lie group...      1\n",
       "4       ALMA as the ideal probe of the solar chromosph...      1\n",
       "...                                                   ...    ...\n",
       "152804   The main results presented in this dissertati...      0\n",
       "152805   Fine-grained sketch-based image retrieval (FG...      0\n",
       "152806   We present the derivation of the NNLO two-par...      0\n",
       "152807   The principle of optimism in the face of unce...      0\n",
       "152808   We consider the setting of prediction with ex...      0\n",
       "\n",
       "[152809 rows x 2 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[['text', 'label']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5e8472a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Pre-process dataframe.\n",
    "\"\"\"\n",
    "MAX_VOCAB = 10_000\n",
    "MAX_LENGTH = 200\n",
    "\n",
    "# init text vectorizer\n",
    "vectorize_layer = TextVectorization(\n",
    "    max_tokens=MAX_VOCAB,\n",
    "    standardize='lower_and_strip_punctuation',\n",
    "    split='whitespace',\n",
    "    ngrams=None,\n",
    "    output_mode='int',\n",
    "    output_sequence_length=MAX_LENGTH,\n",
    "    pad_to_max_tokens=False,\n",
    "    vocabulary=None,\n",
    "    idf_weights=None,\n",
    "    sparse=False,\n",
    "    ragged=False,\n",
    "    encoding='utf-8',\n",
    "    name=None,\n",
    ")\n",
    "\n",
    "# create vocabulary\n",
    "vectorize_layer.adapt(df['text'])\n",
    "vocab = vectorize_layer.get_vocabulary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e8f5cff1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(152809, 200) (152809,)\n"
     ]
    }
   ],
   "source": [
    "# vectorize text data (in subsets for memory constraints)\n",
    "X = []\n",
    "y = df['label']\n",
    "\n",
    "subset_size = df.shape[0] // 100\n",
    "for i in range(0, df.shape[0], subset_size):\n",
    "    subset = df['text'][i : i + subset_size]\n",
    "    X.append(vectorize_layer(subset).cpu())\n",
    "\n",
    "X = np.vstack(X)\n",
    "print(X.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d5e8b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LSTM model generator.\n",
    "\"\"\"\n",
    "EMBEDDING_DIM = 128\n",
    "N_HIDDEN = 100\n",
    "OPTIMIZER = 'adam'\n",
    "N_CLASSES = 2\n",
    "\n",
    "import torch\n",
    "class LSTMModel(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LSTMModel, self).__init__()\n",
    "        self.embeddings = torch.nn.Embedding(\n",
    "            num_embeddings=MAX_VOCAB,\n",
    "            embedding_dim=EMBEDDING_DIM,\n",
    "        )\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size=EMBEDDING_DIM,\n",
    "            hidden_size=N_HIDDEN,\n",
    "            num_layers=1,\n",
    "            batch_first=True,\n",
    "        )\n",
    "        self.linear = torch.nn.Linear(N_HIDDEN, N_CLASSES)\n",
    "        self.softmax = torch.nn.Softmax(dim=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embeddings(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.linear(x[:, -1, :])\n",
    "        x = self.softmax(x)\n",
    "        return x\n",
    "\n",
    "def get_model(model_path=None):\n",
    "    model = LSTMModel()\n",
    "    if model_path:\n",
    "        # load existing model\n",
    "        model.load_state_dict(torch.load(model_path))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4d605323",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 508/508 [00:20<00:00, 25.02it/s, epoch=1, loss=0.6294, acc=0.6758]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 508/508 [00:20<00:00, 25.33it/s, epoch=2, loss=0.4775, acc=0.8242]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 508/508 [00:20<00:00, 25.32it/s, epoch=3, loss=0.4849, acc=0.8164]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 508/508 [00:20<00:00, 24.61it/s, epoch=4, loss=0.4167, acc=0.8984]\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 508/508 [00:22<00:00, 22.52it/s, epoch=5, loss=0.4015, acc=0.9102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 87.63%\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Train and evaluate model.\n",
    "\"\"\"\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# create new model\n",
    "model = get_model().to(device)\n",
    "\n",
    "# Convert to torch tensors\n",
    "if not isinstance(X, torch.Tensor) or not isinstance(y, torch.Tensor):\n",
    "    X = torch.tensor(X)\n",
    "    y = torch.tensor(y)\n",
    "    # Convert to one hot encoding\n",
    "    y = torch.nn.functional.one_hot(torch.tensor(y), N_CLASSES).to(torch.float32)\n",
    "\n",
    "# create data splits\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.15,\n",
    "    random_state=777,\n",
    ")\n",
    "\n",
    "# train the model\n",
    "dataset = torch.utils.data.TensorDataset(X_train, y_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "# train the model\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-7)\n",
    "EPOCHS = 5\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    with tqdm(dataloader, postfix={\"epoch\": epoch, \"loss\": 0, \"acc\": 0}) as pbar:\n",
    "        for i, (X_batch, y_batch) in enumerate(pbar):\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            model.train()\n",
    "            model.zero_grad()\n",
    "            y_pred = model(X_batch)\n",
    "            loss = loss_fn(y_pred, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            acc = (y_pred.argmax(dim=1) == y_batch.argmax(dim=1)).float().mean()\n",
    "            if i % 10 == 0:\n",
    "                # pbar.set_postfix({\"epoch\": epoch, \"loss\": loss.item(), \"acc\": acc.item()})\n",
    "                pbar.set_postfix_str(f\"epoch={epoch}, loss={loss.item():.4f}, acc={acc.item():.4f}\")\n",
    "\n",
    "# final evaluation of the model\n",
    "model.eval()\n",
    "dataset = torch.utils.data.TensorDataset(X_test, y_test)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False)\n",
    "correct, total = 0, 0\n",
    "for X_batch, y_batch in dataloader:\n",
    "    X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "    y_pred = model(X_batch)\n",
    "    correct += (y_pred.argmax(dim=1) == y_batch.argmax(dim=1)).sum().item()\n",
    "    total += y_batch.size(0)\n",
    "accuracy = correct / total\n",
    "# report results\n",
    "print(\"Accuracy: %.2f%%\" % (accuracy * 100))\n",
    "\n",
    "# save model\n",
    "torch.save(model.state_dict(), \"model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "fee60e55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We consider a system of many polymers in solution that interact via an external force that is applied to each pair of polymers. We study the statistical equilibrium of this system, and find that the polymers form clusters whose sizes are given by a power law distribution. This is in contrast to the traditional picture of polymers in solution, where the thermodynamic equilibrium is described by a mean-field theory based on the solution of the mean-field Boltzmann equation. We show that this difference is due to a breakdown of the assumptions that were used to derive the mean-field theory. In particular, we show that the polymer-polymer interactions in the system considered are non-local, and are thus not described by the mean-field theory. We then derive a new theory for the statistical equilibrium in the presence of an external force, which includes a correction to the mean-field theory. The new theory predicts that the polymer clusters become less dense as the external force increases, in clear contrast to the predictions of the mean-field theory. We analyze this disagreement, and show that it is due to the fact that the mean-field theory predicts a non-monotonic dependence of the polymer-polymer interaction strength on the external force, while our theory predicts a strictly monotonic dependence. We then consider the limit of our theory as the number of polymers in the system tends to infinity, and show that it describes a model of polymer quantum mechanics in a Box, which is a system with a large number of infinitely-lived polymers that interact via a non-local potential, and are in statistical equilibrium in the presence of an external force. We analyze this model, and show that it describes a system with anomalous diffusion and ballistic transport, which is analogous to the anomalous behavior observed in recent experiments on pedestrians.\n",
      "1\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "-1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\range.py:391\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    390\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 391\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_range\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnew_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    392\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[1;31mValueError\u001b[0m: -1 is not in range",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [33]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentence, label, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Example machine generated\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m sentence \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtext\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[0;32m      8\u001b[0m label \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlabel\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28mprint\u001b[39m(sentence, label, sep\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\series.py:981\u001b[0m, in \u001b[0;36mSeries.__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m    978\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[key]\n\u001b[0;32m    980\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m key_is_scalar:\n\u001b[1;32m--> 981\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    983\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_hashable(key):\n\u001b[0;32m    984\u001b[0m     \u001b[38;5;66;03m# Otherwise index.get_value will raise InvalidIndexError\u001b[39;00m\n\u001b[0;32m    985\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    986\u001b[0m         \u001b[38;5;66;03m# For labels that don't resolve as scalars like tuples and frozensets\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\series.py:1089\u001b[0m, in \u001b[0;36mSeries._get_value\u001b[1;34m(self, label, takeable)\u001b[0m\n\u001b[0;32m   1086\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_values[label]\n\u001b[0;32m   1088\u001b[0m \u001b[38;5;66;03m# Similar to Index.get_value, but we do not fall back to positional\u001b[39;00m\n\u001b[1;32m-> 1089\u001b[0m loc \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1090\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex\u001b[38;5;241m.\u001b[39m_get_values_for_loc(\u001b[38;5;28mself\u001b[39m, loc, label)\n",
      "File \u001b[1;32mc:\\Program Files\\Python310\\lib\\site-packages\\pandas\\core\\indexes\\range.py:393\u001b[0m, in \u001b[0;36mRangeIndex.get_loc\u001b[1;34m(self, key, method, tolerance)\u001b[0m\n\u001b[0;32m    391\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_range\u001b[38;5;241m.\u001b[39mindex(new_key)\n\u001b[0;32m    392\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[1;32m--> 393\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[0;32m    394\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n\u001b[0;32m    395\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n",
      "\u001b[1;31mKeyError\u001b[0m: -1"
     ]
    }
   ],
   "source": [
    "# Example human"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
